# Multi-GPU Scheduler

A simplified, reliable multi-GPU job scheduling system for distributed computing environments.

## Overview

The Multi-GPU Scheduler is a lightweight job scheduling system designed to manage GPU resources efficiently. This version prioritizes **simplicity, stability, and maintainability** over complex features, providing a clean foundation for GPU resource management.

## System Architecture

### Current Implementation

**Note**: This system currently supports **single-node operation** with **localhost execution**. Multi-node support requires additional node agent implementation.

#### Available Components

1. **Simple Master Server** (`mgpu_simple_master.py`)
   - **Currently working** implementation
   - Handles job queuing and local execution
   - Socket-based API for client communication
   - Supports localhost GPU allocation

2. **Simple Node Agent** (`mgpu_simple_node.py`)
   - **Reference implementation** for remote nodes
   - Can be deployed on compute nodes
   - Communicates with master server
   - **Requires manual setup** for multi-node clusters

3. **Client Interface** (`mgpu_simple_client.py`)
   - **Currently working** command-line client
   - Simple interface: submit, queue, cancel
   - Works with the simple master server

#### Future Development

4. **Enhanced Master Server** (`mgpu_master_server.py`)
   - **Under development** - simplified architecture
   - Will replace complex legacy system
   - Currently incomplete

5. **Enhanced Client** (`mgpu_srun_multinode.py`)
   - **Under development** - improved interface
   - Will provide better user experience
   - Currently incomplete

## Key Features

### Simplified Design Philosophy
- **Clean codebase**: Easy to understand and maintain
- **Minimal dependencies**: Reduces complexity and potential issues
- **Proven patterns**: Based on working simple implementations
- **Robust error handling**: Graceful failure recovery

### Resource Management
- **GPU allocation**: Automatic assignment based on availability
- **Node management**: Support for single and multi-node setups
- **Resource cleanup**: Automatic cleanup on job completion
- **Localhost-first**: Works out of the box on single machines

### Job Management
- **Simple submission**: Submit jobs with minimal configuration
- **Queue status**: Real-time view of job and resource status
- **Job control**: Cancel and monitor running jobs
- **Status tracking**: Clear job state management

## Quick Start

### Prerequisites

- Python 3.7+
- PyYAML
- Basic CUDA setup (for GPU jobs)

### Installation

```bash
git clone <repository-url>
cd multigpu_scheduler
pip install -r requirements.txt
```

### Virtual Environment Setup

For PyTorch and GPU-enabled workloads, use a dedicated virtual environment:

```bash
# Activate the virtual environment (example path)
source venv/bin/activate

# Verify PyTorch installation
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')"
```

**Note**: Replace `venv/bin/activate` with your actual virtual environment path.

### Basic Usage

**Currently working with the simple implementation:**

#### Step 1: Start the Master Server

```bash
# Using virtual environment Python
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

#### Step 2: Start Node Agent (for multi-node or local execution)

```bash
# Start node agent with proper node ID
venv/bin/python src/mgpu_simple_node.py --node-id node001 --master-host 127.0.0.1 --master-port 8080
```

#### Step 3: Submit Jobs

**Interactive job with real-time output (recommended for testing):**
```bash
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python /path/to/your_script.py"
```

**Non-interactive job (background execution):**
```bash
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 "venv/bin/python /path/to/your_script.py"
```

**Example with PyTorch test:**
```bash
# Interactive PyTorch test execution
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"
```

#### Step 4: Monitor and Manage Jobs

```bash
# Check queue status
venv/bin/python src/mgpu_simple_client.py queue

# Cancel a job
venv/bin/python src/mgpu_simple_client.py cancel <job_id>
```

**Job submission with timeout options:**
```bash
# Interactive job with custom timeouts
venv/bin/python src/mgpu_simple_client.py submit --interactive --gpus 1 \
  --session-timeout 3600 \
  --max-consecutive-timeouts 60 \
  "venv/bin/python long_running_script.py"

# Non-interactive job with custom timeouts
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 \
  --max-wait-time 600 \
  --connection-timeout 15 \
  "venv/bin/python batch_job.py"
```

3. **Check queue status:**
```bash
venv/bin/python src/mgpu_simple_client.py queue
```

4. **Cancel a job:**
```bash
venv/bin/python src/mgpu_simple_client.py cancel <job_id>
```

### Multi-node Setup (Manual Node Connection)

**How multiple nodes connect to the master server:**

The current working implementation (`mgpu_simple_*`) supports multi-node setups through manual node agent deployment:

1. **Start master server on the head node:**
```bash
# Using virtual environment with proper host binding
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

2. **On each worker node, connect to the master:**
```bash
# Replace <MASTER_IP> with actual master server IP
venv/bin/python src/mgpu_simple_node.py --master-host <MASTER_IP> --master-port 8080 --node-id node001
venv/bin/python src/mgpu_simple_node.py --master-host <MASTER_IP> --master-port 8080 --node-id node002
# ... repeat for each worker node
```

3. **Submit jobs from any client:**
```bash
# Client can run from any machine that can reach the master
venv/bin/python src/mgpu_simple_client.py --host <MASTER_IP> --port 8080 submit --gpus 2 "venv/bin/python distributed_script.py"
```

**Key Points:**
- **Node agents must be started manually** on each worker machine
- **Master server coordinates** all GPU resources across connected nodes
- **Automatic load balancing** - jobs are assigned to available GPUs across all connected nodes
- **Network accessibility** - all nodes must be able to reach the master server's IP and port
- **Virtual environment consistency** - ensure the same Python environment is available on all nodes

### Tested Example: PyTorch GPU Workload

Here's a complete working example that has been tested:

1. **Start the system:**
```bash
# Terminal 1: Start master server
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080

# Terminal 2: Start node agent
venv/bin/python src/mgpu_simple_node.py --node-id node001 --master-host 127.0.0.1 --master-port 8080
```

2. **Submit and monitor PyTorch test:**
```bash
# Terminal 3: Submit interactive PyTorch test
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"
```

**Expected output:**
```
Job submission: {'status': 'ok', 'job_id': 'F68FAFEC', 'message': 'Job submitted'}
Starting interactive session...
==================================================
PyTorch CUDA Load Test
Start time: 2025-07-28 22:32:49

==================================================
CUDA Environment Check
==================================================
PyTorch version: 2.7.1+cu126
CUDA available: True
CUDA version: 12.6
Number of available GPUs: 1
GPU 0: NVIDIA GeForce RTX 3060
GPU 0 memory: 12.0 GB
...
```

3. **Check system status:**
```bash
# Check queue and node status
venv/bin/python src/mgpu_simple_client.py queue
```

## Configuration

### Single-node Configuration (Current)

For **localhost-only** operation (current working setup):

**No configuration file needed** - the simple master server works out of the box:

```bash
# Step 1: Start master server
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080

# Step 2: Start local node agent
venv/bin/python src/mgpu_simple_node.py --node-id node001 --master-host 127.0.0.1 --master-port 8080

# Step 3: Submit jobs
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python your_script.py"
```

The system will automatically detect available GPUs on localhost.

### Multi-node Configuration (Advanced Setup)

**For advanced users** wanting to set up multiple nodes:

1. **Start master server on main node:**
```bash
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

2. **Start node agents on compute nodes:**
```bash
# On node 1 (ensure virtual environment is available)
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node001

# On node 2  
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node002
```

3. **Configure firewall** to allow communication on port 8080

**Important Requirements:**
- Same virtual environment must be accessible on all nodes
- Python executable path should be consistent: `venv/bin/python`
- Network connectivity between all nodes
- Shared filesystem (optional but recommended for job scripts)

## Usage Examples

### Job Submission (Current Working Commands)

```bash
# Simple single-GPU job
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 "venv/bin/python train.py"

# Multi-GPU job on localhost
venv/bin/python src/mgpu_simple_client.py submit --gpus 2 "venv/bin/python distributed_train.py"

# Interactive job with output streaming (recommended for testing)
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python interactive_script.py"

# Interactive job with custom timeouts for long-running GPU workloads
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive \
  --session-timeout 14400 \
  --max-consecutive-timeouts 120 \
  "venv/bin/python gpu_training.py"

# Non-interactive job with custom monitoring timeouts
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 \
  --max-wait-time 1800 \
  --connection-timeout 20 \
  "venv/bin/python batch_processing.py"

# Real tested example: PyTorch CUDA test
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"
```

### Queue Management

```bash
# View all jobs and node status
venv/bin/python src/mgpu_simple_client.py queue

# Cancel a specific job
venv/bin/python src/mgpu_simple_client.py cancel <job_id>
```

### Advanced: Multi-node Jobs

**ë‹¤ë¥¸ ë…¸ë“œì˜ GPUì— job ìš”ì²­í•˜ëŠ” ë°©ë²•:**

#### 1. ìë™ ìŠ¤ì¼€ì¤„ë§ (ì¶”ì²œ)
ë§ˆìŠ¤í„° ì„œë²„ê°€ ìë™ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ GPUì— í• ë‹¹:
```bash
# ë‹¨ì¼ GPU job - ìë™ìœ¼ë¡œ ê°€ì¥ ì‚¬ìš© ê°€ëŠ¥í•œ ë…¸ë“œì— í• ë‹¹
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 "venv/bin/python script.py"

# ë‹¤ì¤‘ GPU job - ì—¬ëŸ¬ ë…¸ë“œì— ìë™ ë¶„ì‚°
venv/bin/python src/mgpu_simple_client.py submit --gpus 4 "venv/bin/python distributed_training.py"

# ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œë¡œ ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì‹¤í–‰
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"
```

#### 2. íŠ¹ì • ë…¸ë“œ/GPU ì§€ì • (ê³ ê¸‰ ì‚¬ìš©ì)
```bash
# íŠ¹ì • ë…¸ë“œì˜ íŠ¹ì • GPUì— job ì œì¶œ
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --node-gpu-ids "node001:0" "venv/bin/python script.py"

# íŠ¹ì • ë…¸ë“œì˜ ì—¬ëŸ¬ GPU ì‚¬ìš©
venv/bin/python src/mgpu_simple_client.py submit --gpus 2 --node-gpu-ids "node001:0,1" "venv/bin/python multi_gpu_script.py"

# ì—¬ëŸ¬ ë…¸ë“œì— ê±¸ì¹œ ë¶„ì‚° job
venv/bin/python src/mgpu_simple_client.py submit --gpus 4 --node-gpu-ids "node001:0,1;node002:0,1" "venv/bin/python distributed_training.py"

# íŠ¹ì • ë…¸ë“œì—ì„œ ì¸í„°ë™í‹°ë¸Œ ì‹¤í–‰
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive --node-gpu-ids "node002:0" "venv/bin/python debug_script.py"
```

#### 3. ì‹¤ì œ ë©€í‹°ë…¸ë“œ ì„¤ì • ì˜ˆì‹œ

**ë§ˆìŠ¤í„° ë…¸ë“œ (192.168.1.100):**
```bash
# ë§ˆìŠ¤í„° ì„œë²„ ì‹œì‘ - ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì‹ 
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

**ì›Œì»¤ ë…¸ë“œë“¤:**
```bash
# ë…¸ë“œ 1 (192.168.1.101)
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node001

# ë…¸ë“œ 2 (192.168.1.102) 
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node002

# ë…¸ë“œ 3 (192.168.1.103)
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node003
```

**í´ë¼ì´ì–¸íŠ¸ì—ì„œ job ì œì¶œ:**
```bash
# ì–´ëŠ ë¨¸ì‹ ì—ì„œë“  ë§ˆìŠ¤í„° ì„œë²„ì— ì—°ê²°í•˜ì—¬ job ì œì¶œ ê°€ëŠ¥
venv/bin/python src/mgpu_simple_client.py --host 192.168.1.100 --port 8080 submit --gpus 2 "venv/bin/python distributed_script.py"

# íŠ¹ì • ë…¸ë“œ ì§€ì •
venv/bin/python src/mgpu_simple_client.py --host 192.168.1.100 --port 8080 submit --gpus 1 --node-gpu-ids "node002:0" "venv/bin/python script.py"
```

#### 4. ë…¸ë“œ ìƒíƒœ í™•ì¸
```bash
# ëª¨ë“  ë…¸ë“œì™€ GPU ìƒíƒœ í™•ì¸
venv/bin/python src/mgpu_simple_client.py --host 192.168.1.100 --port 8080 queue

# ì¶œë ¥ ì˜ˆì‹œ:
# Jobs:
#   F68FAFEC: running on node002:gpu0
#   A23B4C5D: queued (waiting for 2 GPUs)
# 
# Nodes:
#   node001: 2 GPUs (1 free, 1 busy)
#   node002: 1 GPU (0 free, 1 busy) 
#   node003: 4 GPUs (4 free, 0 busy)
```

## Command Reference

### mgpu_simple_client.py (Current Working Client)

**submit** - Submit a new job
```bash
submit [--gpus N] [--interactive] [--node-gpu-ids "node:gpu_list"] \
       [--session-timeout SECONDS] [--connection-timeout SECONDS] \
       [--max-wait-time SECONDS] [--max-consecutive-timeouts COUNT] \
       "command"
```

**Timeout Options:**
- `--session-timeout`: Maximum session duration in seconds (default: 7200 = 2 hours)
- `--connection-timeout`: Socket connection timeout in seconds (default: 30)
- `--max-wait-time`: Maximum wait time for job output in seconds (default: 300 = 5 minutes)
- `--max-consecutive-timeouts`: Maximum consecutive timeouts before giving up (default: 30)

**queue** - Show queue status
```bash
queue
```

**cancel** - Cancel a job
```bash
cancel <job_id>
```

### mgpu_simple_master.py (Current Working Server)

**Start the master server:**
```bash
venv/bin/python src/mgpu_simple_master.py [--host HOST] [--port PORT]

# Example: Bind to all interfaces on port 8080
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

### mgpu_simple_node.py (For Multi-node Setup)

**Start a node agent:**
```bash
venv/bin/python src/mgpu_simple_node.py --master-host HOST --master-port PORT --node-id NODE_NAME

# Example: Connect to local master
venv/bin/python src/mgpu_simple_node.py --master-host 127.0.0.1 --master-port 8080 --node-id node001
```

## Development

### Architecture Principles

1. **Simplicity First**: Keep code simple and readable
2. **Reliability**: Prefer stable, proven approaches
3. **Maintainability**: Easy to debug and extend
4. **Testability**: Clear interfaces for testing

### Code Structure

**Currently working implementations:**
```
src/
â”œâ”€â”€ mgpu_simple_master.py      # âœ… Working master server
â”œâ”€â”€ mgpu_simple_client.py      # âœ… Working client interface  
â”œâ”€â”€ mgpu_simple_node.py        # âœ… Working node agent
â”œâ”€â”€ mgpu_master_server.py      # ğŸš§ Under development (simplified)
â”œâ”€â”€ mgpu_srun_multinode.py     # ğŸš§ Under development (enhanced client)
â””â”€â”€ mgpu_*.py                  # ğŸ“š Legacy complex implementations
```

**Recommended for new users:** Start with `mgpu_simple_*` files - they are stable and fully functional.

### Testing

```bash
# Basic functionality test with virtual environment
venv/bin/python test/test_torch_load.py

# Test through the scheduler (recommended)
# 1. Start master and node first, then:
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"

# Run test suite
venv/bin/python test/run_tests.py
```

### Quick Test Procedure

**Verified working setup:**

1. **Start the system:**
```bash
# Terminal 1
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080

# Terminal 2  
venv/bin/python src/mgpu_simple_node.py --node-id node001 --master-host 127.0.0.1 --master-port 8080
```

2. **Test PyTorch functionality:**
```bash
# Terminal 3
venv/bin/python src/mgpu_simple_client.py submit --gpus 1 --interactive "venv/bin/python test/test_torch_load.py"
```

3. **Verify output includes:**
   - PyTorch version and CUDA availability
   - GPU device information
   - Successful completion of all test phases

## Troubleshooting

### Node ì—°ê²° ë¬¸ì œ ì§„ë‹¨

**ë‹¤ë¥¸ ë…¸ë“œì—ì„œ ì—°ê²°ì´ ì•ˆë  ë•Œ ë‹¨ê³„ë³„ í™•ì¸ ë°©ë²•:**

#### 1. ë§ˆìŠ¤í„° ì„œë²„ ìƒíƒœ í™•ì¸ (ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ)
```bash
# ë§ˆìŠ¤í„° ì„œë²„ í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep mgpu_simple_master

# í¬íŠ¸ ë°”ì¸ë”© í™•ì¸ - 0.0.0.0:8080 ìœ¼ë¡œ ë°”ì¸ë”©ë˜ì–´ì•¼ í•¨
netstat -tlnp | grep 8080
# ë˜ëŠ”
ss -tlnp | grep 8080

# ê¸°ëŒ€í•˜ëŠ” ì¶œë ¥: tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN
# ë§Œì•½ 127.0.0.1:8080 ìœ¼ë¡œë§Œ ë°”ì¸ë”©ë˜ì–´ ìˆë‹¤ë©´ ì™¸ë¶€ ì—°ê²° ë¶ˆê°€
```

#### 2. ë„¤íŠ¸ì›Œí¬ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì›Œì»¤ ë…¸ë“œì—ì„œ)
```bash
# ë§ˆìŠ¤í„° ë…¸ë“œ ping í…ŒìŠ¤íŠ¸
ping -c 3 192.168.1.100

# í¬íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸
telnet 192.168.1.100 8080
# ë˜ëŠ”
nc -v 192.168.1.100 8080

# ì„±ê³µì‹œ: Connected to 192.168.1.100
# ì‹¤íŒ¨ì‹œ: Connection refused / Connection timed out
```

#### 3. ë°©í™”ë²½ í™•ì¸
**ë§ˆìŠ¤í„° ë…¸ë“œì—ì„œ:**
```bash
# Ubuntu/Debian
sudo ufw status
sudo ufw allow 8080

# CentOS/RHEL/Rocky
sudo firewall-cmd --list-all
sudo firewall-cmd --permanent --add-port=8080/tcp
sudo firewall-cmd --reload

# ë°©í™”ë²½ ì™„ì „ ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ìš©)
sudo ufw disable  # Ubuntu
sudo systemctl stop firewalld  # CentOS
```

#### 4. ë§ˆìŠ¤í„° ì„œë²„ ì¬ì‹œì‘ (ì˜¬ë°”ë¥¸ ë°”ì¸ë”©ìœ¼ë¡œ)
```bash
# ì˜ëª»ëœ ì‹œì‘ (ì™¸ë¶€ ì—°ê²° ë¶ˆê°€)
venv/bin/python src/mgpu_simple_master.py --host 127.0.0.1 --port 8080

# ì˜¬ë°”ë¥¸ ì‹œì‘ (ëª¨ë“  ì¸í„°í˜ì´ìŠ¤ì—ì„œ ìˆ˜ì‹ )
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080
```

#### 5. ë…¸ë“œ ì—ì´ì „íŠ¸ ì—°ê²° í…ŒìŠ¤íŠ¸
**ì›Œì»¤ ë…¸ë“œì—ì„œ:**
```bash
# ìƒì„¸í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ì™€ í•¨ê»˜ ë…¸ë“œ ì—ì´ì „íŠ¸ ì‹œì‘
venv/bin/python src/mgpu_simple_node.py \
  --master-host 192.168.1.100 \
  --master-port 8080 \
  --node-id node001 \
  --verbose

# ì—°ê²° ì„±ê³µì‹œ: "Connected to master server"
# ì—°ê²° ì‹¤íŒ¨ì‹œ: "Connection refused" ë˜ëŠ” "Connection timeout"
```

#### 6. ë„¤íŠ¸ì›Œí¬ ê²½ë¡œ ë° ë¼ìš°íŒ… í™•ì¸
```bash
# ë¼ìš°íŒ… í…Œì´ë¸” í™•ì¸
ip route

# íŠ¹ì • IPë¡œì˜ ê²½ë¡œ ì¶”ì 
traceroute 192.168.1.100

# ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ í™•ì¸
ip addr show
```

#### 7. í¬íŠ¸ ìŠ¤ìº”ìœ¼ë¡œ ì„œë¹„ìŠ¤ í™•ì¸
```bash
# nmapìœ¼ë¡œ ë§ˆìŠ¤í„° ë…¸ë“œ í¬íŠ¸ í™•ì¸
nmap -p 8080 192.168.1.100

# ì¶œë ¥ ì˜ˆì‹œ:
# 8080/tcp open  http-proxy  (ì—°ê²° ê°€ëŠ¥)
# 8080/tcp filtered http-proxy  (ë°©í™”ë²½ ì°¨ë‹¨)
# 8080/tcp closed http-proxy  (ì„œë¹„ìŠ¤ ë¯¸ì‹¤í–‰)
```

#### 8. ë¡œê·¸ ë° ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
```bash
# ë§ˆìŠ¤í„° ì„œë²„ ë¡œê·¸ (í„°ë¯¸ë„ì—ì„œ í™•ì¸)
# ë…¸ë“œ ì—°ê²°ì‹œ ë³´ì´ëŠ” ìƒì„¸ ë©”ì‹œì§€:
# ğŸ”— Node node001 connected from 192.168.1.101:8081
#    â””â”€ ğŸ® 2 GPU(s) detected:
#       â”œâ”€ GPU 0: NVIDIA GeForce RTX 3060 (12288 MB)
#       â”œâ”€ GPU 1: NVIDIA GeForce RTX 4090 (24576 MB)

# ë…¸ë“œ ì—ì´ì „íŠ¸ ë¡œê·¸ (ê° ì›Œì»¤ ë…¸ë“œì—ì„œ)
# âœ… Registration successful
# Simple Node Agent node001 started on 0.0.0.0:8081

# ë…¸ë“œ ì—ì´ì „íŠ¸ ì˜¤ë¥˜ ë©”ì‹œì§€ í™•ì¸
# - "Connection refused": ë§ˆìŠ¤í„° ì„œë²„ ë¯¸ì‹¤í–‰ ë˜ëŠ” í¬íŠ¸ ë¬¸ì œ
# - "No route to host": ë„¤íŠ¸ì›Œí¬ ì—°ê²° ë¬¸ì œ
# - "Connection timed out": ë°©í™”ë²½ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì§€ì—°
# - "Registration failed": ë§ˆìŠ¤í„° ì„œë²„ ì—°ê²°ì€ ë˜ì§€ë§Œ ë“±ë¡ ì‹¤íŒ¨
```

#### 9. ë‹¨ê³„ë³„ ë¬¸ì œ í•´ê²°
```bash
# Step 1: ë§ˆìŠ¤í„° ì„œë²„ ì˜¬ë°”ë¥¸ ì‹œì‘
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080

# Step 2: ë°©í™”ë²½ ì„¤ì • (ë§ˆìŠ¤í„° ë…¸ë“œ)
sudo ufw allow 8080

# Step 3: ì—°ê²° í…ŒìŠ¤íŠ¸ (ì›Œì»¤ ë…¸ë“œ)
telnet 192.168.1.100 8080

# Step 4: ë…¸ë“œ ì—ì´ì „íŠ¸ ì‹œì‘ (ì›Œì»¤ ë…¸ë“œ)
venv/bin/python src/mgpu_simple_node.py --master-host 192.168.1.100 --master-port 8080 --node-id node001

# Step 5: ì—°ê²° í™•ì¸ (ì–´ëŠ ë…¸ë“œì—ì„œë“ )
venv/bin/python src/mgpu_simple_client.py --host 192.168.1.100 --port 8080 queue
```

#### 10. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ì™€ í•´ê²°ì±…
```bash
# "Connection refused"
â†’ ë§ˆìŠ¤í„° ì„œë²„ê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì˜ëª»ëœ ë°”ì¸ë”©
â†’ í•´ê²°: --host 0.0.0.0 ìœ¼ë¡œ ë§ˆìŠ¤í„° ì„œë²„ ì¬ì‹œì‘

# "No route to host"
â†’ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë¬¸ì œ ë˜ëŠ” IP ì£¼ì†Œ ì˜¤ë¥˜
â†’ í•´ê²°: ping í…ŒìŠ¤íŠ¸ ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸

# "Connection timed out"
â†’ ë°©í™”ë²½ì´ í¬íŠ¸ë¥¼ ì°¨ë‹¨í•˜ê³  ìˆìŒ
â†’ í•´ê²°: ë°©í™”ë²½ì—ì„œ 8080 í¬íŠ¸ í—ˆìš©

# "Name or service not known"
â†’ í˜¸ìŠ¤íŠ¸ëª… í•´ì„ ì‹¤íŒ¨
â†’ í•´ê²°: IP ì£¼ì†Œ ì§ì ‘ ì‚¬ìš© ë˜ëŠ” /etc/hosts ì„¤ì •
```

### ê¸°íƒ€ ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **Server won't start**
   - Check if port 8080 is available: `netstat -tlnp | grep 8080`
   - Verify Python environment: `which python`
   - Check dependencies: `pip list | grep -i yaml`

2. **Jobs not running**
   - Verify GPU availability: `nvidia-smi`
   - Check CUDA installation: `nvcc --version`
   - Check node registration: `venv/bin/python src/mgpu_simple_client.py queue`

3. **Virtual environment issues**
   - Ensure same venv path on all nodes
   - Check PyTorch installation consistency
   - Verify CUDA libraries availability

### Debug Mode

Enable detailed logging:
```bash
# For current working implementation
venv/bin/python src/mgpu_simple_master.py --host 0.0.0.0 --port 8080 --verbose

# Add verbose output to node agent
venv/bin/python src/mgpu_simple_node.py --master-host IP --master-port 8080 --node-id nodeXXX --verbose
```

## Migration Guide

### From Complex v1.0 System

If you're migrating from the previous complex system:

1. **Backup your current setup**
2. **Update configuration** to the new simplified YAML format
3. **Test with simple jobs** before production use
4. **Review and simplify** your job submission scripts

### Configuration Changes

- Simplified YAML structure
- Removed complex scheduling options
- Focus on core functionality

## Contributing

1. Follow the simplicity principle
2. Write clear, readable code
3. Add tests for new features
4. Update documentation

## Version History

### v2.0.0 - Simplified Architecture
- Complete rewrite focusing on simplicity and reliability
- Removed complex features that caused stability issues
- Clean, maintainable codebase
- Improved error handling and recovery

### v1.0.0 - Complex System (Legacy)
- Feature-rich but complex architecture
- Multiple threading and streaming features
- Known stability and maintenance issues

## License

[Specify your license here]

## Support

For issues:
1. Check this README and troubleshooting section
2. Review system logs for errors
3. Test with the simple reference implementations
4. Report issues with clear reproduction steps

---

**Note**: This simplified version prioritizes stability and maintainability. If you need advanced features, consider implementing them incrementally on top of this solid foundation.
